# MCP Filesystem Assistant with Web UI

This project is a local AI assistant built using:

- [Model Context Protocol (MCP)](https://github.com/modelcontextprotocol/servers)
- OpenAI GPT-4o
- An MCP-compatible filesystem server
- A React web frontend

It allows you to query and analyze files stored locally via a chat interface.

---

## âœ¨ Features

âœ… Ask questions about files in `/workspace`  
âœ… Automatically read file contents with GPT analysis  
âœ… Web-based chat interface  
âœ… Modular backend architecture  
âœ… Expandable to Azure, Kubernetes, and more

---

## ğŸ“‚ Project Structure

```
Custom-MCP-Server/
â”œâ”€â”€ agent/                  # Node agent (GPT + tools logic)
â”‚   â””â”€â”€ index.js
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ filesystem/
â”‚       â””â”€â”€ server.js       # MCP-compatible Filesystem Server
â”œâ”€â”€ workspace/              # Files you want the agent to access
â”‚   â””â”€â”€ test.txt
â”œâ”€â”€ web/                    # React frontend
â”‚   â””â”€â”€ src/
â”‚       â””â”€â”€ components/
â”‚           â””â”€â”€ ChatInterface.js
â”œâ”€â”€ server.js               # Express backend exposing /chat
â”œâ”€â”€ .env
â”œâ”€â”€ package.json
```

---

## âš™ï¸ Prerequisites

- Node.js v18+
- An OpenAI API key

---

## ğŸ›  Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/your-username/mcp-assistant
   cd mcp-assistant
   ```

2. Install dependencies (root):

   ```bash
   npm install
   ```

3. Install dependencies (frontend):

   ```bash
   cd web
   npm install
   ```

4. Create a `.env` file in the root:

   ```env
   OPENAI_API_KEY=your_openai_api_key_here
   MCP_FILESYSTEM_URL=http://localhost:4001
   ```

---

## ğŸš€ Running the App

Open **3 terminals**:

**Terminal 1 â€“ MCP Filesystem Server**

```bash
cd tools/filesystem
node server.js
```

**Terminal 2 â€“ Express Backend**

```bash
cd Custom-MCP-Server
node server.js
```

**Terminal 3 â€“ React Frontend**

```bash
cd web
npm start
```

Frontend: [http://localhost:3000](http://localhost:3000)  
Backend: [http://localhost:4000](http://localhost:4000)

---

## ğŸ§  How It Works

1. You ask a question in the web chat.
2. The backend (`server.js`) calls `runPrompt()`.
3. GPT determines if it needs to call a tool (`read_file` or `list_files`).
4. The backend performs the tool call and feeds results back into GPT.
5. GPT returns the final answer.
6. The frontend displays it.

---

## ğŸ’¡ Tips

- Ensure all files you want to query are inside `/workspace`.
- Use `setupProxy.js` or a full URL (`http://localhost:4000/chat`) in `fetch()` to avoid proxy issues.
- Persist conversation history (`messages[]`) if you want contextual memory.

---

## âœ… Next Steps

- Add authentication
- Persist chat history
- Deploy to Azure or Kubernetes
- Support CSV and tabular analysis

---

## License

MIT License
